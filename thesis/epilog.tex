\chapter*{Conclusion}
\addcontentsline{toc}{chapter}{Conclusion}

Over the past years, implementing general clustering algorithms using GPUs has become an expanding topic. As there is always more data to process, GPU implementations help greatly in accelerating a related research.  Still, there are many of~the algorithms that are supported only on CPUs, which can significantly decrease their usability. In the present thesis we have proposed and tested an implementation of the Mahalanobis-average hierarchical clustering analysis accelerated on a single GPU. 

For the proposed implementation, we used CUDA platform to communicate with a GPU. We employed high throughput operations like warp shuffle instructions and utilized a device memory hierarchy using shared and constant memory. With~the~stated features, we have achieved remarkable performance improvements.

The proposed MHCA implementation provides performance increase of up to 5000x in measured single-point datasets and 20x in apriori datasets. Moreover, it requires only linear space with respect to the dataset size, which is an~important factor in the general clustering scalability.

With such performance increase, this implementation is able to extend the size of reasonably computable datasets from hundreds of thousands to millions of points. It can be used to advance research in the field of cell cytometry by~decreasing computation time as well as enabling to compute larger datasets.

\subsection*{Future work}

The experiments showed that the measure of dissimilarity between small clusters may be too naive and can propagate a creation of unwanted clusters. This weakness can be addressed by implementing a different dissimilarity measure for clusters under the Mahalanobis threshold. For example, we can perform a transformation on a covariance matrix of a small cluster to make it regular. Then it can be properly inverted and used in the Mahalanobis distance formula.

Moreover, a covariance matrix can be normalized by dividing each element by its discriminant. When used in the distance formula, this transformation produces the combination of the Mahalanobis and Euclidean distance.

The work can be further expanded by implementing the Full Mahalanobis distance (see def.~\ref{def01:fmd}). It can cover the cases when CMD does not provide precise dissimilarity measure which can lead to a better clustering results. Naturally, it would come for the price of~a~greater overall time complexity. 