\chapter{GPU implementation}

This chapter guides the reader through the implementation of Mahalanobis-based hierarchical clustering analysis. First, we describe the algorithm and high level look on implementation. Then, we thoroughly describe the most important functions.

\section{The algorithm choice}

The first chapter introduced three views on implementation of hierarchical clustering. To decide which one to choose we need to take into account two major conditions. First, the algorithm shall not require quadratic \emph{space complexity} as it must be able to process rather large datasets. Second, the algorithm should exhibit some \emph{parallelism opportunities}; otherwise massive GPU parallel properties will be to no use.  We describe mentioned algorithms according to this conditions:
\begin{description}

\item[HCA with dissimilarity matrix] satisfies the first condition if the dissimilarity matrix will not be held in the memory. The second condition is satisfied as each matrix field can be computed independently exposing great parallelism opportunity. 

\item[HCA with the nearest neighbor array] is required sub-quadratic space complexity by definition. On the other hand, it provides less means of parallelism as the algorithm need not compute whole dissimilarity matrix as in the previous algorithm. However, large datasest sizes may compensate for this disadvantage. 

\item[HCA with priority queues] is unusable as it does not satisfy the first condition. Moreover, operation over priority queues (insert, delete and min) are not of a parallel nature and may create bottleneck in the computation.

\end{description}

For this thesis we chose to implement HCA with the nearest neighbor array. It promises better time complexity than the HCA with dissimilarity matrix which can come to a great use when clustering big datasets. In addition, we utilized the advantages of priority queues by defining a constant that declares the number of closest neighbors assigned to each cluster.

Additionally, we chose to implement the CMD variant of Mahalanobis distance to fully exhibit GPU parallel properties.

\subsection{Apriori clusters}

When cluster sizes reach the Mahalanobis threshold, a MHCA algorithm starts to form elliptic shaped clusters. However, until then, simple euclidean distance is used. This is an important stage of the algorithm as an improper initial clustering can decrease the overall clustering result. Hence, a simple hierarchical euclidean clustering can be unsatisfactory.

To solve this problem, we implemented additional feature to the algorithm called \emph{apriori clusters} (see def. \ref{def03:apriori}). The intended usage of the feature is that the apriori clusters are created from a dataset using non-hierarchical clustering algorithm such as \emph{k-means} --- the algorithm that would be capable of creating better initial clusters that a MHCA algorithm. 

With this, a MHCA algorithm is guided by the apriori clusters to create plenty of small clusters that just reached the Mahalanobis threshold; these clusters can now fully exhibit the MHCA properties. Overall, we suppose that we can lessen the probability of an error during the early stage of the algorithm.

\begin{defn}
	Suppose a dataset $\mathcal{D}$, a HCA algorithm $\mathcal{A}$ and a partitioning of  $\mathcal{D}$ into non-empty subsets $\mathcal{D}_1,\dots,\mathcal{D}_k$. If following conditions hold:
	\begin{enumerate}
		\item $\mathcal{A}$ performs clustering of each subset separately. Meaning that clusters from different subsets can not be merged.
		\item When each subset is clustered into the only cluster, $\mathcal{A}$ clusters the resulting clusters as if they were in one subset.
	\end{enumerate}
	then we call the subsets $\mathcal{D}_1,\dots,\mathcal{D}_k$ \emph{apriori clusters} of $\mathcal{A}$.
	\label{def03:apriori}
\end{defn}



\section{Class \texttt{hierarchical\_clustering}}
The above mentioned algorithm is implemented in the main class of the program --- \texttt{gmhc}\footnote{An abrevation for GPU Mahalanobis-based hierarchical clustering.}. It inherits from an abstract template class \texttt{hierarchical\_clustering}.
This template provides key data fields and methods for a hierarchical clustering algorithm (see list. \ref{lst03:hc}): 

\begin{description}
	\item[\texttt{initialize()}] sets the fields of the class. A templated field \texttt{points} is an array of dataset objects --- points. They are expected to be of a floating-point numeric type. Fields \texttt{point\_dim} and \texttt{points\_size} state a point dimensionality and a total number of points in the array. Hence, each \texttt{point\_dim} consecutive array elements represent one point and the total array size being $\texttt{point\_dim}*\texttt{points\_size}$ elements.
	
	\item[\texttt{run()}] initiates the clustering. It returns a vector of \texttt{pasgn\_t} structures --- pairs that contain \emph{assignments} --- the IDs of merged clusters. Each point from the input array is assigned an unique consecutive ID starting from $0$. When two clusters are merged, the new cluster is assigned the next available unique ID. The process of merging is then stored in the returning vector. Hence, the vector completely describes the whole clustering process.
	
	\item[\texttt{free()}] deallocates all acquired resources.
\end{description}

\begin{lstlisting}[caption={A summary of \texttt{hierarchical\_clustering} header file.},label={lst03:hc}]
using csize_t = uint32_t;
using asgn_t = csize_t;
using pasgn_t = std::pair<asgn_t, asgn_t>;

template <typename T>
class hierarchical_clustering
{
protected:
	const T* points;
	csize_t points_size;
	csize_t point_dim;
public:
	virtual void initialize(...);
	virtual std::vector<pasgn_t> run() = 0;
	virtual void free() = 0;
};
\end{lstlisting}

\section{Class \texttt{gmhc}}

This class is the main entrypoint of the whole MHCA algorithm. It inherits from \texttt{hierarchical\_clustering<float>} template specialization, which means that the algorithm expects dataset objects to be single-precision points of a specified dimension. The class communicates
with a GPU; hence, it holds structures used by the device.

It contains an array of \texttt{clustering\_context\_t} objects. Each of them holds the context of a specific part of the dataset and is able to perform a clustering on it.

\subsection{Initialization}

Initialization happens in the \texttt{initialize} method. Due to the initialization of device and host structures, this stage must be treated with importance.
The class overloads \texttt{initialize} method so that it sets three additional fields:
\begin{itemize}
	\item \texttt{mahalanobis\_threshold} indicates the threshold in the \emph{Altered general distance} (see def. \ref{def01:alt}).
	
	\item\texttt{apriori\_assignments}  is an array of assignments that splits \texttt{points} into \emph{apriori clusters}. This is an optional field of the \texttt{initialize} method.
	
	\item \texttt{validator} is an optional field used for testing purposes.
\end{itemize}

Next follows the main responsibility of the \texttt{initialize} method --- device and host data allocation and initialization. Device data are used within kernels while host data control the flow of run kernels. They are \emph{point}, \emph{assignment} \emph{centroid}, \emph{inverse covariance}, \emph{neighbor} and \emph{status arrays} (see defs. \ref{def03:point}, \ref{def03:assign}, \ref{def03:tuple}). 

All but the status array are device arrays. The \emph{indexing structure} indicates a location of a cluster data distributed among the cluster-based arrays (see defs. (\ref{def03:tuple}, \ref{def03:index})). The most importantly, data about a specific cluster resides under the same index within all cluster-related arrays.

\begin{defn}
	Suppose a dataset $\mathcal{D}$ that contains objects represented by $k$-di\-men\-si\-onal ponts. We define \emph{point array} $\mathcal{P}$ as an array that contains consecutive sequence of all objects in $\mathcal{D}$.
	\label{def03:point}
\end{defn}

\begin{defn}
	Suppose a partitioning of $\mathcal{P}$ of size $n$ into clusters $c_1,\dots,c_k$ where each cluster $c_i$ is defined by its unique id $id_i$ and a set of indices $I_i$ that state which points from $\mathcal{P}$ belong to the cluster. The array of length $n$ is \emph{assignments array} $\mathcal{A}$ when for each cluster $c_i$ the following equation holds.
	$${\forall j \in I_i : \mathcal{A}_j = id_i}$$
	\label{def03:assign}
\end{defn}

\begin{defn}
	Suppose a dataset $\mathcal{D}$, its clusters $c_1,\dots,c_k$,  Mahalanobis threshold $T_M$ and a positive number $m$. We the define the following \emph{cluster-based arrays}:
	\begin{itemize}
		\item \emph{centroid array} $\mathcal{C}$ as an array that contains centroids of all $k$ clusters,
		\item \emph{inverse covariance array} $\mathcal{I}$ as an array that contains inverse covariance matrices of all clusters that reached $T_M$,
		\item \emph{neighbor array} $\mathcal{N}$ as an array that contains indices and distances to the $m$ closest neighbors of all $k$ clusters,
		\item \emph{status array} $\mathcal{S}$ as an array that contains sizes and unique ids of all $k$ clusters.
	\end{itemize}
	\label{def03:tuple}
\end{defn}

\begin{defn}
	Suppose a dataset $\mathcal{D}$, its $k$ clusters and a corresponding cluster-based array tuple $(\mathcal{C},\mathcal{I},\mathcal{N},\mathcal{S})$. We define \emph{indexing structure} $\mathcal{Q}$ that determines the order of the cluster-related data in this arrays.
	\label{def03:index}
\end{defn}

\subsection{Clustering}
The clustering algorithm resides in the method \texttt{run}. This method utilizes the array of \texttt{clustering\_context\_t} objects --- instances of a \emph{clustering context} (see def. \ref{def03:context}). If \texttt{apriori\_assignments} are present, each element of the array is the clustering context of a corresponding apriori cluster and holds methods for its further clustering. 

\begin{defn}
	Suppose a dataset $\mathcal{D}$ and its subset $\mathcal{D}'$. We define the \emph{clustering context} of $\mathcal{D}'$ as a tuple $(\mathcal{P},\mathcal{A},\mathcal{C},\mathcal{I},\mathcal{N},\mathcal{S})$ where each array contains data of points and clusters created within $\mathcal{D}'$.
	\label{def03:context}
\end{defn}

In addition to that, the method uses one special clustering context called the \emph{final context}. If apriori clusters are present (and the above mentioned array is not empty), it is unset and serves as a context that will host all the apriori cluster results for the final clustering. If there are no apriori clusers, the final context is set and contains the whole dataset.

We divide the whole clustering algorithm into a top level \emph{apriori clustering} and a low level \emph{context clustering}.

\begin{description}
	\item[Apriori clustering] handles a separate clustering of apriori clusters, merging their results into one clustering context and performing the final clustering (see alg. \ref{alg03:apriori}).
	
	The algorithm iterates over each apriori clusters from the context array. It perfroms its clustering using context clustering method and the result is inserted into the final context. Either when all apriori clusters are merged or when there are none at all, the final context with the remaining clusters is clustered and the list of assignment pairs is returned.
	
	Before any context begins the clustering, its closest neighbor array has to be initialized. It is done by the kernel \texttt{run\_neighbors}.
	
	
	\begin{algorithm}
		\caption{Apriori clustering}
		\label{alg03:apriori}
		\begin{algorithmic}[1]
			\Procedure{apriori}{$\mathcal{C}, ctx_f$}
			\ForAll{$ctx \in \mathcal{C}\cup ctx_f$} \Comment{iterate over apriori clusters and the final context}
			\State initialize the closest neighbor array of $ctx$ \Comment{kernel \texttt{run\_neighbors}}
			\While{$ctx$ has not the only cluster}
			\State iterate clustering of $cxt$ for the closest pair $(i,j)$  \Comment{\emph{context clustering}}
			\State store $(i,j)$ into the returning list
			\EndWhile
			\State $ctx_f \gets ctx_f \cup ctx$ \Comment{assign merged cluster into the final context}
			\EndFor
			\State \textbf{return} list of merged clusters
			\EndProcedure
		\end{algorithmic}
	\end{algorithm}

	\item[Context clustering] performs one iteration of clustering within the specified context. First, it goes through the neighbor array of the context and finds the closest pair of clusters (kernel \texttt{neighbor\_min}). The clusters are then merged into the new one; data structures are updated to conform this action. 
	
	Remainder of the algorithm handles consequences of the new cluster. First, assignments of \texttt{points} array are recomputed (kernel \texttt{merge\_clusters}). Then the centroid of the new cluster is computed and stored (\texttt{centroid} kernel). Last, if the new cluster reaches Mahalanobis threshold, its inverse covariance matrix is computed (kernel set \texttt{compute\_icov}). 
	
	The final statement of the method updates the closest neighbor array according to the one created and two deleted clusters (\texttt{update\_neighbors} kernel). It sustains the consistency when the method is visited again.  
	
	\begin{algorithm}
	\caption{Context clustering}
	\label{alg03:context}
	\begin{algorithmic}[1]
		\Procedure{context}{}
		\State retrieve the clostest pair $(i,j)$ from the neighbor array
		\State merge clusters $(i,j)$ into the new cluster $c$
		\State update assignments of points into clusters
		\State compute the centroid of $c$
		\If{$|c| \ge \texttt{mahalanobis\_threshold}$}
		\State compute the inverse covariance matrix of $c$
		\EndIf
		\State update the closest neighbor array
		\State \textbf{return} $(i,j)$
		\EndProcedure
	\end{algorithmic}
\end{algorithm}

\end{description}





 
