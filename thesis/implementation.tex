\chapter{GPU implementation}

This chapter guides the reader through the implementation of Mahalanobis-based hierarchical clustering analysis. First, we describe the algorithm and high level look on implementation. Then, we thoroughly describe the most important functions.

\section{The Algorithm}

The first chapter introduced three views on implementation of hierarchical clustering. To decide which one to choose we need to take into account two major conditions. First, the algorithm shall not require quadratic \emph{space complexity} as it must be able to process rather large datasets. Second, the algorithm should exhibit some \emph{parallelism opportunities}; otherwise massive GPU parallel properties will be to no use.  We describe mentioned algorithms according to this conditions:
\begin{description}

\item[HCA with dissimilarity matrix] satisfies the first condition if the dissimilarity matrix will not be held in the memory. The second condition is satisfied as each matrix field can be computed independently exposing great parallelism opportunity. 

\item[HCA with the nearest neighbor array] is required sub-quadratic space complexity by definition. On the other hand, it provides less means of parallelism as the algorithm need not compute whole dissimilarity matrix as in the previous algorithm. However, large datasest sizes may compensate for this disadvantage. 

\item[HCA with priority queues] is unusable as it does not satisfy the first condition. Moreover, operation over priority queues (insert, delete and min) are not of a parallel nature and may create bottleneck in the computation.

\end{description}

For this thesis we chose to implement HCA with the nearest neighbor array. It promises better time complexity than the HCA with dissimilarity matrix which can come to a great use when clustering big datasets. In addition, we utilized the advantages of priority queues by defining a constant that declares the number of closest neighbors assigned to each cluster.

Additionally, we chose to implement the CMD variant of Mahalanobis distance to fully exhibit GPU parallel properties.

\subsection{Apriori clusters}

When cluster sizes reach the Mahalanobis threshold, a MHCA algorithm starts to form elliptic shaped clusters. However, until then, simple euclidean distance is used. This an important stage of the algorithm as an improper initial clustering can decrease the overall clustering result. Hence, a simple hierarchical euclidean clustering can be unsatisfactory.

To solve this problem, we implemented additional feature to the algorithm called \emph{apriori clusters} (see def. \ref{def02:apriori}). The intended usage of the feature is that the apriori clusters are created from a dataset using non-hierarchical clustering algorithm such as \emph{k-means} --- the algorithm that would be capable of creating better initial clusters that a MHCA algorithm. 

With this, a MHCA algorithm is guided by the apriori clusters to create plenty of small clusters that just reached the Mahalanobis threshold; these clusters can now fully exhibit the MHCA properties. Overall, we suppose that we can lessen the probability of an error during the early stage of the algorithm.

\begin{defn}
	Suppose a dataset $\mathcal{D}$, a HCA algorithm $\mathcal{A}$ and a partitioning of  $\mathcal{D}$ into non-empty subsets $a_1,\dots,a_k$. If following conditions hold:
	\begin{enumerate}
		\item $\mathcal{A}$ performs clustering of each subset separately. Meaning that clusters from different subsets can not be merged.
		\item When each subset is clustered into the only cluster, $\mathcal{A}$ clusters the resulting clusters as if they were in one subset.
	\end{enumerate}
	then we call the subsets $a_1,\dots,a_k$ \emph{apriori clusters} of $\mathcal{A}$.
	
	\label{def02:apriori}
\end{defn}

\section{specs of machine}

code was optimized for this machine

\section{impl description} 

high level look

describe each kernel 