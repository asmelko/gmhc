\chapter{GPU implementation}

This chapter focuses on the implementation of the Mahalanobis-average hierarchical clustering analysis. First, we summarize the most important parts of the CUDA parallel platform. Next, we describe the algorithm and the~high level look on the implementation. Last, we thoroughly describe the~most important functions.

\section{CUDA programming model overview}
\label{sec02:cuda}

The problem this thesis aims to solve is to implement an algorithm that is able to provide a hierarchical clustering of very large datasets while retaining a reasonable computation time --- the time required to compute a magnitude smaller datasets with current HCA algorithms.
In an effort of achieving this performance, we used a combination of C++ programming language and \emph{CUDA}\footnote{Compute Unified Device Architecture} API.

CUDA is a parallel platform and API allowing a programmer to use GPU for~general purpose programming. This API exposes a computational power of~hundreds (even thousands) cores of CUDA-enabled GPUs~\cite{cuda}.


\subsection{Terminology}

The starting point for running any code on GPU using CUDA is a \emph{kernel}. A~kernel is a function that is executed on GPU; we say it contains \emph{device code}. Complementary to a device code, a \emph{host code} is a phrase for code executed on a CPU. Hence, a common CUDA application runs host code that determines which device code to run next. 

A kernel is run $n$ times in parallel by $n$ threads each having its unique \emph{thread ID}. IDs of threads can be identified by \emph{one-dimensional}, \emph{two-dimensional} or \emph{three-dimensional} indices forming a block of threads, called \emph{thread block}. This property reflects shapes of common structures such as vectors and matrices resulting in~more natural programming work.

Since all block threads reside on the same processor and share common resources, the block size is limited\footnote{Currently up to 1024 threads}. However, a kernel can be launched with multiple equally shaped blocks to increase the number of running threads. They can be organized into up to three-dimensional structure called \emph{grid}. This naturally implies unique block ID. A grid can be of an arbitrary size; usually dictated by~the~computed data (see fig.~\ref{fig02:grid}). It is a common practice that grid size surpasses the number of GPU processors.


\begin{figure}\centering
	\includegraphics[width=7cm]{img/grid}
	\caption{An example of 2D grid of size (3,2) with 2D blocks of size (4,3) (taken from the CUDA C++ Programming Guide).}
	\label{fig02:grid}
\end{figure}


The device memory is hierarchically divided into parts each being accessible by a different set of threads:

\begin{description}
	\item[Global memory] -- This memory is accessible by any thread. Any memory request is transferred via transactions; hence, to avoid decrease of the data throughput, memory accesses should be coalesced. 
	\item[Local memory] -- Each thread has exclusive access to its local memory. As it resides in a global memory, the local memory is a thread private global memory.
	\item[Shared memory] -- The shared memory is assigned to a block. It means that threads from the same block have access to the same shared memory. It is placed on-chip so it has much lower latency and higher bandwidth than the global or local memory.
	\item[Constant memory] -- The constant memory is a read-only memory accessible by any thread. Due to its read-only property, it can be heavily cached and may perform better than global memory. Together with global memory, it persists across different kernel launches.
\end{description}

The building block of the CUDA-enabled GPU hardware architecture is multi-threaded \emph{streaming multiprocessor (SM)}. A GPU contains an array of multiprocessors whose number varies between different GPUs. When a kernel launches, its grid of blocks is distributed among available multiprocessors and execute in parallel (see fig.~\ref{fig02:SM}). On the selected multiprocessor, block threads are executed in~parallel as well. Moreover, multiple blocks can run concurrently on one multiprocessor.

\begin{figure}\centering
	\includegraphics[width=8cm]{img/SM}
	\caption{An example of different grid block distributions among SMs (taken from the CUDA C++ Programming Guide).}
	\label{fig02:SM}
\end{figure}

To achieve this grade of parallelism, a GPU employs the \emph{SIMT architecture} (Single Instruction - Multiple Threads). A~multiprocessor partitions an assigned block to chunks of 32 threads called \emph{warps}. Each warp thread is executed concurrently. During the execution, threads start with the the same program address but have own registers and instruction pointers so they can branch independently. However, a warp executes one common instruction at the time. Hence, to achieve the greatest performance all warp threads must agree on the program path.

Moreover, as warp threads execute concurrently, CUDA offers \emph{warp shuffle instructions}. All threads in a warp are able to distribute their data to another thread within just one instruction. This comes to a great use for synchronization but can also be used to achieve high throughput reduction operations as well.

\subsection{GPU performance and optimization concerns}

To greatly utilize a CUDA-enabled GPU and achieve a good performance, a program code must be written with respect to the GPU architecture. These guidelines help us in this task:

A GPU task has to be split to many small data dependent sub-tasks (e.g.~a computation of one element in a dissimilarity matrix). Threads must not perform many different tasks. Rather, they must perform the same task with a different data. Satisfying this condition, we follow the model of the SIMT architecture.

The programmer should minimize data transfers with low bandwidth such as copying data between CPU and GPU. This can be accomplished by creating and operating with structures directly in a GPU without mapping it to the CPU memory.


On-chip shared memory should be utilized. It is equivalent to the user-ma\-na\-ged cache and provides much higher data throughput than the global memory. 
Heavy use task related data should be moved there.

Last, the application should maximize the instruction throughput. It is accomplished by using lower precision data types that does not affect the result or by using high throughput instructions like warp shuffle. The most importantly, the programmer should minimize the use of control flow instructions (if, switch, while, etc.) that cause divergent warps. 


\section{Implementation strategy}
\label{sec03:impl}

Section \ref{sec01:hca} introduced three variants of a hierarchical clustering analysis. To decide which one to implement, it must satisfy the memory usage and the level of parallelism. Specifically, the algorithm must have subquadratic space requirements as it must be able to process rather large datasets. Next, the algorithm should exhibit some parallelism opportunities; otherwise massive GPU parallel properties will be to no use.  We compare the mentioned algorithms according to these conditions:

\begin{description}

\item[In-place HCA] satisfies the memory usage with its linear space complexity. In this variant, finding the most similar cluster pair is equivalent to a computation of the whole dissimilarity matrix. The parallelism requirement is satisfied because each measure of dissimilarity can be computed independently, exposing great parallelism opportunity. 

\item[In-place HCA with the nearest neighbor array] satisfies the required memory usage as well. On the other hand, it provides less space for a parallelism because the algorithm does not compute whole dissimilarity matrix as in the previous algorithm. Large dataset sizes may compensate for this disadvantage. 

\item[HCA with priority queues] is unsuitable as it does not satisfy the memory usage. Moreover, operation over priority queues (insert, delete and min) are not of a parallel nature and may create a bottleneck in the computation.

\end{description}

For this thesis we chose to implement HCA with the nearest neighbor array. It promises better time complexity than the HCA with dissimilarity matrix which can come to a great use when clustering big datasets. In addition, we utilized the~advantages of priority queues by defining a constant that declares the~number of~closest neighbors assigned to each cluster (so there can be also the second closest, the third closest, etc.).

Additionally, we decided to implement the CMD variant of the Mahalanobis distance (see def.~\ref{def01:cmd}) as a measure of dissimilarity. It is simpler to implement than FMD and it may provide further knowledge for implementation of the FMD variant.

\subsection{Apriori clustering optimization}

As already mentioned, the present HCA algorithms can hardly cluster datasets of big sizes. This can be improved by implementing the apriori clustering method.

\begin{defn}[Apriori clustering]
	Given a dataset $\mathcal{D}$ and a HCA algorithm $\mathcal{A}$, we define the apriori clusters of $\mathcal{A}$ as a partitioning of  $\mathcal{D}$ into non-empty subsets $\mathcal{D}_1,\dots,\mathcal{D}_k$    if these conditions hold:
	\begin{enumerate}
		\item $\mathcal{A}$ performs clustering of each subset separately; meaning that clusters from different subsets can not be merged.
		\item When each subset is clustered into the only cluster, $\mathcal{A}$ clusters the resulting clusters as if they were in one subset.
	\end{enumerate}
	\label{def03:apriori}
\end{defn}

The intended usage of this method is that the apriori clusters are created from a dataset using non-hierarchical clustering algorithm such as \emph{k-means} --- an algorithm that is capable of creating a dataset partitioning fast. With this feature, a HCA algorithm is guided by the apriori clusters to~create the corresponding merged clusters. They do not cluster the whole dataset at once. Rather, each apriori cluster is clustered separately. This results in a much faster computation (see section \ref{sec04:apr_perf}) and increased size of computable datasets. 

\begin{figure}\centering
	\includegraphics[width=10cm]{img/apriori_example}
	\caption{An example dataset with (right) and without (left) visualized pre-computed apriori clusters (each apriori cluster is visualized as an ellipse --- dataset points contained in an ellipse represent a subset of the dataset).}
	\label{fig03:apr_ex}
\end{figure}

\section{Benchmarking framework}

We implemented the selected algorithm with the apriori clustering method in C++ language. Its implementation resides in the main class of the program --- \texttt{gmhc}\footnote{An abbreviation for GPU Mahalanobis-average Hierarchical Clustering}. It inherits from an abstract template class \texttt{hierarchical\_clustering}.
This template provides key data fields and methods for a hierarchical clustering algorithm (see list.~\ref{lst03:hc}): 

\begin{description}
	\item[\texttt{initialize()}] sets the fields of the class. A templated field \texttt{points} is an array of dataset objects --- points. They are expected to be of a floating-point numeric type. Fields \texttt{point\_dim} and \texttt{points\_size} state a point dimensionality and a total number of points in the array. Hence, each \texttt{point\_dim} consecutive array elements represent one point and the total array size is~$\texttt{point\_dim}\cdot\texttt{points\_size}$ elements.
	
	\item[\texttt{run()}] initiates the clustering. It returns a vector of \texttt{pasgnd\_t} structures --- pairs that contain \emph{assignments} and a \emph{distance} --- the IDs of merged clusters and~the~distance between them. Each point from the input array is assigned an~unique consecutive ID starting from $0$. When two clusters are merged, the new cluster is assigned the next available unique ID. The process of merging is then stored in the returning vector. Hence, the vector completely describes the whole clustering process.
	
	\item[\texttt{free()}] deallocates all acquired resources.
\end{description}

\begin{lstlisting}[caption={A summary of \texttt{hierarchical\_clustering} header file.},label={lst03:hc}]
using csize_t = uint32_t;
using asgn_t = csize_t;
using pasgn_t = std::pair<asgn_t, asgn_t>;
template <typename T>
using pasgnd_t = std::pair<pasgn_t, T>;

template <typename T>
class hierarchical_clustering
{
protected:
	const T* points;
	csize_t points_size;
	csize_t point_dim;
public:
	virtual void initialize(...);
	virtual std::vector<pasgnd_t<T>> run() = 0;
	virtual void free() = 0;
};
\end{lstlisting}

\section{Mahalanobis clustering implementation}
\label{sec03:gmhc}

The class \texttt{gmhc} is the entry point of the whole gpu-accelerated MHCA algorithm. It inherits from \texttt{hierarchical\_clustering<float>} template specialization, which means that the algorithm expects dataset objects to be single-precision points of~a~specified dimension. The class communicates
with a GPU; hence, it holds structures used by the device.

\subsection{Initialization}

The initialization happens in the \texttt{initialize} method. Due to the initialization of~device and host structures, this stage must be treated with importance.
The~class overloads \texttt{initialize} method so that it sets three additional fields:
\begin{itemize}
	\item \texttt{mahalanobis\_threshold} indicates the threshold of the Non-singular Mahalanobis distance (see def.~\ref{def01:alt}).
	
	\item\texttt{apriori\_assignments}  is an array of assignments that splits \texttt{points} into \emph{apriori clusters}. This is an optional field of the \texttt{initialize} method.
	
	\item \texttt{validator} is an optional field used for testing purposes.
\end{itemize}

Next follows the main responsibility of the \texttt{initialize} method --- device and host data allocation and initialization. They are \emph{point}, \emph{assignment}, \emph{centroid}, \emph{inverse covariance}, \emph{neighbor} and \emph{status arrays} $\mathcal{P}$, $\mathcal{A}$, $\mathcal{C}$, $\mathcal{I}$, $\mathcal{N}$, $\mathcal{S}$. 

\begin{defn}[Point array]
	Suppose a dataset $\mathcal{D}$ that contains objects represented by $k$-di\-men\-si\-onal points. We define point array $\mathcal{P}$ as an array that contains consecutive sequence of all objects in $\mathcal{D}$.
	\label{def03:point}
\end{defn}

\begin{defn}[Assignment array]
	Suppose a partitioning of a point array $\mathcal{P}$ into~clusters $c_1,\dots,c_k$ where each cluster $c_i$ is defined by its unique ID $id_i$ and a set of indices $I_i$ that state which points from $\mathcal{P}$ belong to the cluster. The array of length $|\mathcal{P}|$ is the assignments array $\mathcal{A}$ when for each cluster $c_i$ the following equation holds:
	$${\forall j \in I_i : \mathcal{A}_j = id_i}$$
	\label{def03:assign}
\end{defn}

\begin{defn}[Cluster-related arrays]
	Given a dataset $\mathcal{D}$, its clusters $c_1,\dots,c_k$, a~Mahalanobis threshold $T_M$ and a positive number $m \le |\mathcal{D}|$, we define the following cluster-related arrays:
	\begin{itemize}
		\item the centroid array $\mathcal{C}$ as an array that contains centroids of all $k$ clusters,
		\item the inverse covariance array $\mathcal{I}$ as an array that contains inverse covariance matrices of all clusters that reached $T_M$,
		\item the neighbor array $\mathcal{N}$ as an array that contains $m$ indices and distances to~the~$m$~closest clusters for each of $k$ clusters,
		\item the status array $\mathcal{S}$ as an array that contains sizes and unique ids of all $k$~clusters.
	\end{itemize}
	\label{def03:tuple}
\end{defn}


All but the status array are device arrays. Device arrays are used within kernels while the host array controls the flow of run kernels.  


\subsection{Clustering}
The clustering algorithm resides in the method \texttt{run}. This method utilizes the~array of \texttt{clustering\_context\_t} objects --- instances of a \emph{clustering context} --- that divide the dataset according to the provided apriori clusters (see fig.~\ref{fig03:clust_ctx}).

\begin{defn}[Clustering context]
	Given a dataset $\mathcal{D}$, its point array $\mathcal{P}$, assignment array $\mathcal{A}$ and cluster-related array tuple $(\mathcal{C}, \mathcal{I}, \mathcal{N}, \mathcal{S})$, we define the clustering context of $\mathcal{D}' \subset \mathcal{D}$ as a tuple $(\mathcal{P'}, \mathcal{A'}, \mathcal{C'}, \mathcal{I'}, \mathcal{N'}, \mathcal{S'})$. In the tuple, each array is a sub-array of the corresponding array of $\mathcal{D}$. Each sub-array contains data of points and clusters created within $\mathcal{D}'$.
	\label{def03:context}
\end{defn}

\begin{figure}\centering
	\includegraphics[width=\linewidth]{img/clustering_context}
	\caption{An example of a dataset division into subsets and creation of clustering contexts (each context is assigned the sub-arrays of the main arrays with data relevant only to the subset).}
	\label{fig03:clust_ctx}
\end{figure}

The algorithm proceeds as follows.
If \texttt{apriori\_assignments} are present, the clustering context array is initialized and filled with clustering contexts. Each of them describes the data structure of the corresponding apriori cluster and holds methods for its further clustering. It will be further called the \emph{context array}. 

In addition to that, the method \texttt{run} uses one special clustering context called the \emph{final context}. If apriori clusters are present (and the context array is non-empty), the final context is unset and serves as a context that will host the apriori cluster results for the final clustering. If there are no apriori clusters, the final context is set and~contains the whole dataset.

We divide the whole clustering algorithm into the top level \emph{apriori clustering layer} and the low level \emph{context clustering layer}.

\begin{description}
	\item[Apriori clustering layer] handles a separate clustering of apriori clusters, mer\-ges their results into one clustering context and performing the final clustering (see alg.~\ref{alg03:apriori}).
	
	In this layer, the algorithm iterates over each apriori cluster from the context array. It enters the context clustering layer to perform the clustering. It retrieve the result that is then inserted into the final context. Either when all apriori clusters are merged or when there are none at all, the final context with the remaining clusters is clustered and the list of assignment pairs is returned.
	
	Before any context begins the clustering, its closest neighbor array has to~be initialized. It is done by the kernels \texttt{neighbors}.
	
	
	\begin{algorithm}[t]
		\caption{Apriori clustering}
		\label{alg03:apriori}
		\begin{algorithmic}[1]
			\Procedure{apriori}{context array $\mathcal{T}$, the final context $ctx_f$}
			\ForAll{$ctx \in \mathcal{T}\cup ctx_f$} \Comment{iterate over apriori clusters and the final context}
			\State initialize the closest neighbor array of $ctx$ 
			\While{$ctx$ contains more than one cluster}
			\State perform context clustering of $cxt$ for the closest pair $(i,j)$ and their distance $d$
			\State store $((i,j),d)$ into the returning list
			\EndWhile
			\State $ctx_f \gets ctx_f \cup ctx$ \Comment{assign merged cluster into the final context}
			\EndFor
			\State \textbf{return} list of merged clusters
			\EndProcedure
		\end{algorithmic}
	\end{algorithm}

	\item[Context clustering layer] performs one iteration of clustering within the specified context (see alg.~\ref{alg03:context}). First, it goes through the neighbor array of the context and finds the closest pair of clusters (kernel \texttt{neighbor\_min}). At that moment, there are two clusters to be removed and one new cluster to be created. Hence, the cluster-related arrays of~the~context need to be updated accordingly.
	
	The remainder of this method sets data for the new cluster. First, $\mathcal{A}$ is~recomputed to correctly show that points of the former two clusters now belong to the new one (kernel \texttt{merge\_clusters}). Then, the new centroid is computed and stored into $\mathcal{C}$ (kernel \texttt{centroid}) and status array is updated as well (in the host code). Last, if the new cluster reaches the Mahalanobis threshold, its inverse covariance matrix is computed and stored into $\mathcal{I}$ (kernels \texttt{compute\_icov}). 
	
	The final statement of the context clustering method checks the whole $\mathcal{N}$  according to~the~one created and two deleted clusters and updates it when needed (kernels \texttt{update\_neighbors}).
	
	\begin{algorithm}[t]
	\caption{Context clustering}
	\label{alg03:context}
	\begin{algorithmic}[1]
		\Procedure{context}{$\mathcal{A},\mathcal{C},\mathcal{I},\mathcal{N},\mathcal{S},\mathcal{Q}$}
		\State retrieve the closest pair $(i,j)$ and distance $d$ from $\mathcal{N}$
		\State initialize an unique $id$ of the new cluster $c$
		\State update $\mathcal{Q}$ and retrieve new index $idx$ for $c$
		\State reorder data in $(\mathcal{C},\mathcal{I},\mathcal{N},\mathcal{S})$
		\State update $\mathcal{A}$
		\State $\mathcal{C}_{idx} \gets$ compute the centroid of $c$
		\State $\mathcal{S}_{idx} \gets \{|c|, id\}$
		\If{$|c| \ge \texttt{mahalanobis\_threshold}$}
		\State $\mathcal{I}_{idx} \gets$ compute the inverse covariance matrix of $c$
		\EndIf
		\State update $\mathcal{N}$
		\State \textbf{return} $((i,j),d)$
		\EndProcedure
	\end{algorithmic}
\end{algorithm}

\end{description}


\subsection{Data order}

This section describes the actual order of data in the cluster-related arrays and describesthe process of reorganizing the cluster-related arrays provided that a cluster pair $(i,j)$ is to be merged.

This is achieved with a help of the \texttt{cluster\_bound\_t} structure called the \emph{indexing structure}. It is a rather simple structure that describes how the cluster-related arrays organize their data. The structure distinguishes two parts of the arrays: The first part contains the data of clusters whose size have not reached the Mahalanobis threshold while the second part stores the data of the remaining clusters. Hence, it contains a pair of indices indicating beginnings of the two parts and another pair describing the sizes of~the~parts. We will respectively call them \emph{Euclidean} and \emph{Mahalanobis array parts}. Note, the~structure is uniform for each cluster-related array; meaning that a different array data of~the~same cluster resides on the same index.

When a cluster pair $(i,j)$ is to be merged, the indexing structure first recognizes the parts of an array where the indices $i$, $j$ belong. Hence, it identifies the places of arrays that are no longer valid. It gets rid of the places in the two particular ways:
\begin{itemize}
	\item assigns the invalid place as the place for the new cluster,
	\item moves the end data of the corresponding part to the invalid place.
\end{itemize}
The reason why we organize the data in such way is to maintain continuously distributed data (see fig.~\ref{fig03:data_order}). This data distribution serves a great help for running device kernels.

\begin{figure}\centering
	\includegraphics[width=\textwidth]{img/data}
	\caption{An example of the data distribution in a cluster-related array. Two clusters of non-Mahalanobis size with their indices $i$, $j$ are being merged into a~Mahalanobis-sized cluster on the index $k$.}
	\label{fig03:data_order}
\end{figure}

\begin{rem}
	Naturally, there are other ways to maintain continuously distributed data within arrays. We can move the data in such way that the new cluster will always reside at the beginning of the corresponding part of an array. This approach could decrease the complexity of several kernels. On the other hand, it requires at most a linear time to move the data instead of a constant time.
\end{rem}

\section{CUDA Kernels}

Kernels handle the computation critical parts of the algorithm. They are divided into several groups:

\begin{itemize}
	\item \emph{Clusters merging} updates the assignment array so it reflects the merging of~two clusters.
	\item \emph{Centroid computation} goes through the assignment array and computes the~centroid of the newly formed cluster. 
	\item \emph{Inverse covariance matrix computation} is a set of kernels which compute the~inverse of the covariance matrix of newly formed clusters that reached the Mahalanobis threshold. 
	\item \emph{Minimum retrieval} iterates over the neighbor array to find the closest two clusters.
	\item \emph{Neighbor array update} updates the neighbor array structure with respect to the newly created cluster.
\end{itemize}

Each kernel group reads and modifies a different data array and has its own time complexity. Hence, to complete the kernel summary, we provide this information in the table \ref{tab03:kernels}. The following sections will describe each kernel group in more detail.

 \begin{table}
 	\centering
	\renewcommand{\arraystretch}{1.5}
 	\begin{tabular}{llll}
 		\toprule
 		\textbf{Kernel group}                  & {\textbf{Input arrays}} & \textbf{Output arrays}  & \textbf{Complexity} \\ \midrule
 		\textbf{Clusters merging}              &     $\mathcal{A}$       &     $\mathcal{A}$      &       $O(n)$        \\
 		\textbf{Centroid computation}          &     $\mathcal{A}$, $\mathcal{P}$       &  $\mathcal{C}$  &   $O(n)$\\
 		\parbox{10em}{\textbf{Inverse covariance\\ matrix computation}}            &  $\mathcal{A}$, $\mathcal{P}$, $\mathcal{C}$        &     $\mathcal{I}$                 &       $O(n)$ \\
 		\textbf{Minimum retrieval}             &     $\mathcal{N}$         & --     &       $O(c)$        \\
 		\textbf{Neighbor array update}         &    $\mathcal{C}$,    $\mathcal{I}$, $\mathcal{N}$        &     $\mathcal{N}$      &      $O(c^2)$       \\ \bottomrule
 	\end{tabular}
 	\caption{A summary of the kernel groups ($n$ is a number of points in a clustering context; $c$ is a number of clusters in a clustering context).}
 	\label{tab03:kernels}
 \end{table}

\subsection{Clusters merging}


The kernel \texttt{merge\_clusters} is responsible for a merging of clusters. As the~input, it takes the~assignment array $\mathcal{A}$, a pair of old cluster IDs (of merged clusters) and a new ID (of a created cluster). It goes through the array in a \emph{grid-stride loop} and replaces any of the old IDs with the new ID.

\begin{defn}
	A grid-stride loop iterates over all threads in the grid multiple times until the~desired number of iterations is hit. The following code shows an example of a 1D grid stride loop. The first assignment creates an unique thread index across a grid. The index is added a grid size until the condition fails.
	\label{def03:grid-stride}
\end{defn}

\begin{lstlisting}
for (auto idx = threadIdx.x + blockIdx.x * blockDim.x; 
     idx < iters; 
     idx += gridDim.x * blockDim.x);
\end{lstlisting}

\subsection{Centroid computation}

The kernel \texttt{centroid} computes the centroid of a cluster with a specified $id$. According to the $id$, the kernel filters the point array $\mathcal{P}$ using the assignment array $\mathcal{A}$ to~find the involved points.

Each thread maintains its local array that stores the sum of the points that belong to the cluster. 

First, a grid-stride loop is performed over $\mathcal{P}$. If a point belongs to the cluster (determined using $\mathcal{A}$ and $id$), the thread aggregates it in the sum array. 

After the loop ends, the \emph{shared memory} and \emph{warp shuffle instructions} are utilized to reduce the sum arrays of all block threads into one array (we will call this the~\emph{block reduction}).

Each block reduced array is then divided by the cluster size and stored into $\mathcal{C}$ using the \emph{atomic add instruction}. 

\subsection{Inverse covariance matrix computation}


To compute the inverse covariance matrix of a cluster with an $id$, multiple kernels are run from the host method \texttt{compute\_icov}. 

First, the covariance matrix of the cluster has to be computed via kernels \texttt{covariance} and \texttt{finish\_covariance}. 

Then, the \emph{CUBLAS}\footnote{CUDA implementation of BLAS library} library is used to invert the covariance matrix. If the~matrix can not be inverted (due to the singular property) the inverse is set to~the~default value --- identity. 

Last, a special transformation is performed on the inverse covariance matrix and the result is stored to $\mathcal{I}$ via the \texttt{store\_icovariance} kernel.

\subsubsection{Covariance matrix computation}

The first kernel \texttt{covariance} computes the upper triangular covariance matrix of~a~cluster multiplied by the cluster size. 

To achieve a linear complexity in the number of points, we use the centroid $C$ of the cluster (it is already computed by the previous kernel). Before the kernel starts, $C$ is copied to the \emph{constant memory} so all threads can use it.

At the kernel start, each thread uniformly selects a part of the \emph{shared memory} as its intermediate upper-triangulate covariance matrix. As more threads can share the same part, the atomic access is necessary.

Then, a grid-stride loop over $\mathcal{P}$ with an $id$ filtering follows. If a point belongs to the cluster, a thread subtracts it by $C$; we will denote the subtraction result with $S$. After that, each element $i$ of $S$ is separately multiplied with the element $j \ge i$ and atomically added to the corresponding place of the intermediate covariance matrix of the thread. The equation \ref{eq03:cov} describes the process. There, $\textbf{X}$ denotes the discrete random variable of the x-coordinate of all cluster points, $C_x$ denotes the x-coordinate of $C$ and $\mathbf{S_x}$ denotes discrete random variable $\mathbf{X}-C_x$.

\begin{equation}
\begin{split}
n\cov(\mathbf{X},\mathbf{Y}) &= n\mathbf{E}[(\mathbf{X}-\mathbf{E}(\mathbf{X}))(\mathbf{Y}-\mathbf{E}(\mathbf{Y}))] = n\mathbf{E}[(\mathbf{X}-C_x)(\mathbf{Y}-C_y)] \\ &= n\mathbf{E}[(\mathbf{S_x})(\mathbf{S_y})] = n\frac{1}{n}\sum_{i=1}^{n}{\mathbf{S}_{\mathbf{x}}^{i}\mathbf{S}_{\mathbf{y}}^{i}} = \sum_{i=1}^{n}{\mathbf{S}_{\mathbf{x}}^{i}\mathbf{S}_{\mathbf{y}}^{i}}
\end{split}
\label{eq03:cov}
\end{equation}

After the loop, the \emph{block sum reduction} is performed and the resulting covariances are stored to the \emph{global memory} via the \emph{atomic add instruction}.

\subsubsection{Finishing covariance matrix}
Immediately next follows the kernel \texttt{finish\_covariance}. It takes the result of~the previous kernel as the input, it divides the matrix by the cluster size and~fills the lower triangular part for the inversion operation.

We could incorporate the division operation into the previous kernel. However, it would be needed to be done by each thread performing the atomic operation. Here, each element is divided just once.

\subsubsection{Storing inverse covariance matrix}
After the inversion done by the CUBLAS subroutine \texttt{cublasSmatinvBatched}, the kernel \texttt{store\_icovariance} performs the following actions with its result:
\begin{enumerate}
	\item Transforms the matrix to the upper triangulate variant.
	\item Multiplies the non-diagonal elements of the matrix by a factor $2$.
	\item Stores the transformed matrix into $\mathcal{I}$.
\end{enumerate}

The first step is performed to get rid of redundant information and to save space as the $\mathcal{I}$ array has the greatest space complexity among the cluster-related arrays.

The second step is an optimization for the computation of the Mahalanobis distance equation (see eq. \ref{eq01:maha}). We can allow this optimization as the distance is a \emph{quadratic form} that can be simplified as shown in the equation \ref{eq03:form}.

\begin{equation}
\T{\mathbf{x}}\mathbf{M}\mathbf{x} = \sum_{i=1}^{n}\sum_{j=1}^{n}{m_{ij}x_ix_j} = \sum_{i=1}^{n}{m_{ii}x_i^2} + \sum_{i=1}^{n}\sum_{j>i}^{n}{2m_{ij}x_ix_j}
\label{eq03:form}
\end{equation}


\begin{rem}
The overall time complexity of the \texttt{compute\_icov} function is $O(n)$ as~each kernel needs to iterate over all points in a clustering context. We omitted complexity of computing a point covariance ($O(dim^2)$) and a matrix inversion ($O(dim^3)$) as the size of the point dimension is negligible compared to~the~size of the clustering context.
\end{rem}

\subsection{Minimum retrieval}

The kernel \texttt{neighbor\_min} finds the closest neighbor among all the neighbors in~the neighbor array $\mathcal{N}$.

The loop over $\mathcal{N}$ is performed block-stride as the grid must consist of just one block. That is because there is no grid synchronization in CUDA; meaning that threads can be synchronized only within the same block. In this task, we need to synchronize all involved threads.

Each thread iterates through the loop and stores its local closest neighbors (their distances and indices).

After the loop ends, the threads of the only block perform the block reduction achieving the globally closest neighbors.

\subsection{Neighbor array update}

The host method \texttt{update\_neighbors} is a set of five kernels where each of them takes an important part in updating the neighbor array $\mathcal{N}$.

First, the kernel \texttt{update} collects invalidated indices. Next, two distinct kernels \texttt{neighbors\_u} and \texttt{neighbors\_mat\_u} are run to update the~array --- the first one updates the Euclidean part while the second kernel updates the~Mahalanobis part. Then, the reduction kernel \texttt{reduce\_u} is called that reduces intermediate results generated by the previous two kernels. Last, the kernel \texttt{neighbors\_new} performs a special update of $\mathcal{N}$ for the newly created clusters.

Apriori clustering layer uses the kernel set \texttt{neighbors}. This set is used for the initialization of $\mathcal{N}$; it is a modification of the currently described kernels. There, the \texttt{update} and \texttt{neighbors\_new} kernels are ignored as all elements of the array have to be updated and none of them is created. The~notation of kernel names suggests that kernels suffixed with \texttt{\_u} have a suffix-less variant that processes all elements.

\subsubsection{Update detection kernel}
The kernel \texttt{update} checks the validity of the neighbors of each cluster using a grid-stride loop. The check consists of these steps:
\begin{itemize}
	\item If the index of the closest neighbor belongs to a cluster that was merged, remove the~neighbor.
	\item If the neighbor index belongs to a cluster that was moved during an array reorder, change the index accordingly.
\end{itemize} 
When no neighbors are present, the particular cluster is marked for the update. The index of this cluster is stored in the \emph{update array} $\mathcal{U}$. It is divided into two parts --- same as for the other cluster-related arrays. 

$\mathcal{U}$ to contain continuous indices of update-ready clusters, threads synchronize over the \emph{global memory} variable indicating the next available location of $\mathcal{U}$. The variable is manipuled by threads using the \emph{atomic add instruction}.

\subsubsection{Euclidean update kernel}

The \texttt{neighbor\_u} kernel takes care of updating clusters whose size have not reached the Mahalanobis threshold (we call them \emph{Euclidean clusters} for simplicity). For that reason, the input of the kernel is $\mathcal{C}$ (to compute distances) and $\mathcal{U}$ (to target clusters for the computation).

The first loop iterates through $\mathcal{U}$. All threads iterate the whole loop. Therefore, when a cluster $c$ is selected  from $\mathcal{U}$, all threads contribute to the computation of its closest neighbors.

For each $c \in \mathcal{U}$, threads perform a grid-stride loop over Euclidean clusters. Each thread takes care of one cluster at a time, computes its distance with $c$ and updates its local closest neighbors of $c$.

Then, the threads perform the block reduction and create the block-local closest neighbors of $c$. As there is no grid synchronization, each block stores its result in the~intermediate neighbor array $\mathcal{N}'$ ready for a further reduction.

The kernel ends when all clusters from $\mathcal{U}$ are visited.

\begin{rem}
	As the Non-singular Mahalanobis distance function is symmetric, there is no need to compute a distance between all clusters and a cluster $c$ (the one that needs to~be~updated). We only need to compute distances of clusters with their indices greater than the~index of $c$. If we follow this invariant throughout the~algorithm, we can safely ignore the clusters of lower index in the computation as~during \emph{theirs} computation the~cluster $c$ was already involved. This does not hold for~newly created clusters. They need to be measured against all the remaining clusters to~hold the invariant (see the kernel \texttt{neighbors\_new}).
\end{rem}

\subsubsection{Mahalanobis update kernel} The \texttt{neighbor\_mat\_u} kernel is similar to the previous one. The main distinction is that this kernel updates clusters that reached the Mahalanobis threshold (\emph{Mahalanobis clusters}, for simplicity) instead of those that did not. We created the separate kernel to tailor the thread utilization for the demanding Mahalanobis distance computation.

In addition to $\mathcal{C}$ and $\mathcal{U}$, the kernel takes $\mathcal{I}$ as the input. It is important to access the inverse covariance matrices because they are involved in a distance computation of Mahalanobis clusters. 

Same as in the previous kernel, the first loop is performed by all threads to cooperatively compute the closest neighbors for each element of $\mathcal{U}$.

In contrast to the previous kernel, each warp takes care of one cluster at a time in the grid-stride loop instead of a thread. Warp threads cooperate together in the matrix-vector multiplications as the Mahalanobis distance function is now involved. The previous kernel does not perform a matrix-vector multiplication for computing the distance so only one thread computes one distance measurement.

Finally, the kernel performs the same reduction as in the previous kernel and stores the block-local intermediate results into $\mathcal{N}'$.

\begin{rem}
	The invariant of distance computation holds in this kernel as well. However, when a Mahalanobis cluster $c$ needs to be updated, distances between $c$ and \emph{all} Euclidean clusters must be also computed regardless the index. This is a~trade-off for having Euclidean and Mahalanobis clusters divided from themselves. However, the majority of Euclidean clusters tend to merge themselves early in the clustering.
\end{rem}

\subsubsection{Reduction kernel} The kernel \texttt{reduce\_u} reduces the intermediate results of the updated clusters in~$\mathcal{N}'$ into the~neighbor array $\mathcal{N}$. For each updated cluster, there is as much closest neighbor entries as there were blocks in a grid of the previous kernels. As this may be a~big number, one \emph{warp} cooperates in the reduction of the entries for one updated cluster.

\subsubsection{New cluster update kernel} The \texttt{neighbor\_new} kernel updates $\mathcal{N}$ in the means of the newly created cluster $c$. It computes the remainder of the distances (between $c$ and the clusters of lower index) to satisfy the mentioned invariant. Hence, this kernel moves that responsibility from the kernels \texttt{neighbor\_u} and \texttt{neighbor\_mat\_u} to simplify their code and process the special measures faster. The kernel is called after the reduction kernel as it operates directly with $\mathcal{N}$. 

\begin{rem}
	The time complexity of the \texttt{update\_neighbors} kernels is $O(c^2)$, where $c$ denotes the number of clusters in a clustering context. This is due to the~kernels that compute the closest neighbors. In the worst case, neighbors of~each cluster need to be recomputed resulting in the mentioned time complexity. However, this is rarely the case. More often, the number of update-needed clusters resides in a yet unpredictable but reasonable scope. Hence, when we denote the number of clusters needed to be updated as $u$, we can start reasoning about the more precise time complexity $O(uc)$.
\end{rem}
